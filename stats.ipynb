{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114d467c-b233-4a0c-bf20-bee2270f89b8",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical method used to compare means among three or more groups to determine whether there are statistically significant differences between them. Several assumptions must be met for ANOVA to yield valid results:\n",
    "\n",
    "Independence: Observations within and between groups must be independent of each other. Violations of independence occur when there is dependence between observations, such as in repeated measures designs or when data are clustered.\n",
    "\n",
    "Normality: The data within each group should be approximately normally distributed. Violations of normality occur when the distributions of data within groups are significantly skewed or kurtotic, which may affect the reliability of the results, especially for smaller sample sizes.\n",
    "\n",
    "Homogeneity of Variance (Homoscedasticity): The variance of the data within each group should be approximately equal. This means that the spread of data points around the mean should be similar across all groups. Violations of homogeneity of variance, or heteroscedasticity, occur when the variability differs significantly between groups, leading to unequal variances.\n",
    "\n",
    "Examples of violations that could impact the validity of ANOVA results include:\n",
    "\n",
    "Outliers: Extreme values in the data that do not follow the same pattern as the rest of the data can skew the results, especially if they are present in one group more than others.\n",
    "\n",
    "Non-normality: If the assumption of normality is violated, the p-values and confidence intervals obtained from ANOVA may not be accurate. This can occur if the data are heavily skewed or have heavy tails.\n",
    "\n",
    "Unequal Variances: Violations of homogeneity of variance can lead to inflated Type I error rates (false positives) and reduced power. This often occurs when sample sizes are unequal between groups or when there are differences in variability between groups.\n",
    "\n",
    "Non-Independence: Violations of independence, such as in repeated measures designs or clustered data, can lead to biased estimates of the group means and inflated Type I error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537852a-b084-4f13-af85-a055c86e1ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a61f7605-b36a-4938-bbbd-d469173bccf0",
   "metadata": {},
   "source": [
    "One-way ANOVA: One-way ANOVA is used when there is one categorical independent variable (with three or more levels) and one continuous dependent variable. It is used to determine whether there are statistically significant differences in the means of the dependent variable across the different levels of the independent variable.\n",
    "\n",
    "Example: A researcher wants to compare the effectiveness of three different teaching methods (independent variable) on student test scores (dependent variable). One-way ANOVA would be used to determine if there are significant differences in test scores between the three teaching methods.\n",
    "\n",
    "Two-way ANOVA: Two-way ANOVA is used when there are two categorical independent variables (factors) and one continuous dependent variable. It examines the main effects of each independent variable as well as the interaction between them.\n",
    "\n",
    "Example: A researcher wants to investigate the effects of both gender and treatment type on blood pressure (dependent variable). Two-way ANOVA would allow them to test for main effects of gender and treatment type, as well as the interaction between gender and treatment type on blood pressure.\n",
    "\n",
    "Repeated Measures ANOVA: Repeated Measures ANOVA is used when the same subjects are measured multiple times under different conditions or at different time points. It is used to analyze within-subjects effects and to test for differences between the means of repeated measurements.\n",
    "\n",
    "Example: A psychologist wants to examine the effects of three different therapy techniques on anxiety levels in patients. Each patient undergoes all three therapy techniques, and anxiety levels are measured before and after each session. Repeated Measures ANOVA would be used to analyze whether there are significant differences in anxiety levels across the different therapy techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b23b3b-1851-4ae9-9407-c44b4d195c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa9860ca-f810-4f5b-b3b5-31363e1b6e2a",
   "metadata": {},
   "source": [
    "Partitioning of variance in ANOVA refers to the division of the total variability in the data into different components, each of which represents a different source of variation. Understanding this concept is crucial because it helps researchers understand the relative contributions of different factors to the overall variability in the data, allowing them to draw more accurate conclusions about the effects of the independent variables on the dependent variable.\n",
    "\n",
    "In ANOVA, the total variability in the data is partitioned into three main components:\n",
    "\n",
    "Between-group variance (SSBetween): This component of variance represents the variability in the dependent variable that is due to differences between the group means. It reflects the extent to which the independent variable explains differences in the dependent variable across groups.\n",
    "\n",
    "Within-group variance (SSWithin): Also known as error variance, this component of variance represents the variability in the dependent variable that is not explained by the independent variable. It includes random variability within each group as well as any other sources of variability not accounted for by the model.\n",
    "\n",
    "Total variance (SSTotal): This is the overall variability in the dependent variable across all observations. It is the sum of the between-group and within-group variances.\n",
    "\n",
    "The partitioning of variance is important for several reasons:\n",
    "\n",
    "Assessing the significance of the independent variable: By comparing the between-group variance to the within-group variance, researchers can determine whether the differences observed between groups are statistically significant. If the between-group variance is much larger than the within-group variance, it suggests that the independent variable has a significant effect on the dependent variable.\n",
    "\n",
    "Interpreting the size of effects: Partitioning of variance allows researchers to quantify the proportion of variability in the dependent variable that can be attributed to the independent variable. This helps in interpreting the size of the effects and understanding their practical significance.\n",
    "\n",
    "Identifying sources of variability: By decomposing the total variability into different components, researchers can identify and understand the sources of variability in the data. This information can be used to refine experimental designs, control for confounding variables, and improve the accuracy of statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a283045-ec02-4a6d-9088-7b2d0bddcf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100810be-01f0-4add-98b0-de07be557528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 229.6\n",
      "Explained Sum of Squares (SSE): 7.6\n",
      "Residual Sum of Squares (SSR): 222.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'group1': [10, 12, 15, 18, 20],\n",
    "    'group2': [8, 11, 14, 16, 19],\n",
    "    'group3': [9, 13, 16, 17, 21]\n",
    "}\n",
    "\n",
    "# Convert data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the overall mean\n",
    "overall_mean = np.mean(df.values)\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "SST = np.sum((df.values - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the explained sum of squares (SSE)\n",
    "group_means = df.mean()\n",
    "group_counts = df.count()\n",
    "SSE = np.sum(group_counts * (group_means - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", SST)\n",
    "print(\"Explained Sum of Squares (SSE):\", SSE)\n",
    "print(\"Residual Sum of Squares (SSR):\", SSR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c2ad6-c287-4550-aa19-2fc00bfefb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'group1': [10, 12, 15, 18, 20],\n",
    "    'group2': [8, 11, 14, 16, 19],\n",
    "    'group3': [9, 13, 16, 17, 21],\n",
    "    'factor1': ['A', 'A', 'B', 'B', 'C'],\n",
    "    'factor2': ['X', 'Y', 'X', 'Y', 'X']\n",
    "}\n",
    "\n",
    "# Convert data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('value ~ C(factor1) + C(factor2) + C(factor1):C(factor2)', data=df).fit()\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract main effects and interaction effect\n",
    "main_effects = anova_table['sum_sq'][:-1] / anova_table['sum_sq'][:-1].sum()\n",
    "interaction_effect = anova_table['sum_sq'][-1] / anova_table['sum_sq'][:-1].sum()\n",
    "\n",
    "print(\"Main Effects:\")\n",
    "print(main_effects)\n",
    "print(\"Interaction Effect:\")\n",
    "print(interaction_effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8f717-15b1-4035-abe1-6c66a857bc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c15c817-3e35-4a98-b312-63f99f1a0ff4",
   "metadata": {},
   "source": [
    "In the context of a one-way ANOVA, the F-statistic is used to test whether there are statistically significant differences in the means of the dependent variable across the different groups. The associated p-value tells us the probability of observing such extreme results (or more extreme) under the assumption that the null hypothesis is true (i.e., there are no differences between the group means).\n",
    "\n",
    "Given the provided results:\n",
    "\n",
    "F-statistic: 5.23\n",
    "p-value: 0.02\n",
    "With a small p-value (typically less than a chosen significance level, often 0.05), we reject the null hypothesis. This means that there is evidence to suggest that there are statistically significant differences between the group means.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The F-statistic of 5.23 indicates that there is some degree of variability between the group means relative to the variability within the groups.\n",
    "The p-value of 0.02 indicates that the observed differences between the group means are statistically significant at the 0.05 significance level.\n",
    "Therefore, we can conclude that there are statistically significant differences in the means of the dependent variable across the different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a86f4-7648-469c-af9c-ab5b12268c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d169cd9a-11ae-4e02-9507-03f00fed3a2c",
   "metadata": {},
   "source": [
    "Complete Case Analysis (CCA): This method involves excluding cases with missing data from the analysis. While simple to implement, CCA may lead to biased estimates if the missing data are not missing completely at random (MCAR) or missing at random (MAR), potentially reducing statistical power.\n",
    "\n",
    "Mean Imputation: Mean imputation involves replacing missing values with the mean of the observed values for that variable. While mean imputation is straightforward, it can underestimate the variability in the data and bias the results if the missing data are not MCAR or MAR.\n",
    "\n",
    "Last Observation Carried Forward (LOCF): LOCF involves replacing missing values with the value of the last observed measurement for that variable. LOCF assumes that missing values remain constant over time, which may not always be appropriate.\n",
    "\n",
    "Linear Interpolation: Linear interpolation involves estimating missing values based on the values of neighboring time points. While linear interpolation can provide more accurate estimates than mean imputation or LOCF, it may not be appropriate if the underlying relationship between variables is not linear.\n",
    "\n",
    "Multiple Imputation: Multiple imputation involves generating multiple plausible values for each missing value based on the observed data and then averaging the results across multiple imputed datasets. Multiple imputation provides more accurate estimates than single imputation methods and can account for uncertainty due to missing data.\n",
    "\n",
    "The choice of method for handling missing data in repeated measures ANOVA depends on the assumptions about the missing data mechanism and the goals of the analysis. It is important to assess the potential consequences of different methods for handling missing data, as each method may introduce different biases and affect the validity and interpretability of the results. Additionally, sensitivity analyses can be conducted to evaluate the robustness of the results to different methods for handling missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f20d23-5917-420b-aa61-00701309f377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8bd5a95-b4d5-4355-9c70-bdb3d11bda55",
   "metadata": {},
   "source": [
    "After conducting an ANOVA and finding a significant result, post-hoc tests are often used to determine which specific groups differ from each other. Some common post-hoc tests used after ANOVA include:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD) Test: Tukey's HSD test is widely used when comparing multiple groups. It controls the familywise error rate, allowing for pairwise comparisons between all groups while maintaining a specified overall Type I error rate (usually 0.05). It is appropriate when the number of pairwise comparisons is relatively large.\n",
    "\n",
    "Bonferroni Correction: Bonferroni correction adjusts the significance level for each comparison to maintain an overall familywise error rate. It is a conservative method and is appropriate when the number of comparisons is relatively small.\n",
    "\n",
    "Scheffe's Test: Scheffe's test is a conservative post-hoc test that is suitable for comparing specific contrasts or combinations of means. It is robust and can be used for unequal sample sizes and unequal variances.\n",
    "\n",
    "Dunnett's Test: Dunnett's test is used when comparing multiple treatment groups to a control group. It is less conservative than Bonferroni correction and is suitable for situations where there is a single control group.\n",
    "\n",
    "Games-Howell Test: The Games-Howell test is a non-parametric post-hoc test that does not assume equal variances between groups. It is appropriate when the assumption of homogeneity of variances is violated.\n",
    "\n",
    "Example situation:\n",
    "Suppose a researcher conducts an experiment to compare the effectiveness of four different teaching methods (A, B, C, and D) on student performance in a standardized test. After conducting a one-way ANOVA, they find a significant difference between the groups (p < 0.05). However, the ANOVA does not provide information on which specific groups differ from each other. In this case, the researcher would need to conduct post-hoc tests to determine pairwise differences between the teaching methods. They could use Tukey's HSD test or another appropriate post-hoc test to make these comparisons and identify which teaching methods lead to significantly different outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37edddd2-9486-41f3-865a-36d1256c6056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8723ac-147a-4cc5-abd0-dbbc7ed46beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 30.33425034387895\n",
      "p-value: 1.0149083975678836e-11\n",
      "The p-value is less than 0.05 - Reject the null hypothesis.\n",
      "There are significant differences between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Weight loss data for each diet (A, B, and C)\n",
    "diet_A = [5, 7, 6, 8, 9, 5, 6, 7, 8, 9, 7, 6, 5, 7, 8, 6, 5, 6, 7, 8, 9, 7, 6, 5, 7, 8, 6, 5, 7, 8, 9, 7, 6, 5, 6, 7, 8, 9, 7, 6, 5, 7, 8, 6, 5, 7, 8, 9, 7]\n",
    "diet_B = [4, 6, 5, 7, 8, 4, 5, 6, 7, 8, 6, 5, 4, 6, 7, 5, 4, 5, 6, 7, 8, 6, 5, 4, 6, 7, 5, 4, 6, 7, 8, 6, 5, 4, 5, 6, 7, 8, 6, 5, 4, 6, 7, 5, 4, 5, 6, 7, 8]\n",
    "diet_C = [3, 5, 4, 6, 7, 3, 4, 5, 6, 7, 5, 4, 3, 5, 6, 4, 3, 4, 5, 6, 7, 5, 4, 3, 5, 6, 4, 3, 5, 6, 7, 5, 4, 3, 4, 5, 6, 7, 5, 4, 3, 5, 6, 4, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The p-value is less than\", alpha, \"- Reject the null hypothesis.\")\n",
    "    print(\"There are significant differences between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"The p-value is greater than or equal to\", alpha, \"- Fail to reject the null hypothesis.\")\n",
    "    print(\"There is no significant difference between the mean weight loss of the three diets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36fc2f-ffcc-41bf-a97e-ae70b72e5a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18958d8c-f280-4ed8-940d-852b9c281cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(0)\n",
    "n_employees = 30\n",
    "n_levels = 2  # Novice and experienced\n",
    "n_programs = 3\n",
    "programs = ['A', 'B', 'C']\n",
    "experience_levels = ['Novice', 'Experienced']\n",
    "\n",
    "data = {\n",
    "    'Time': np.random.randint(10, 30, size=n_employees * n_programs * n_levels),\n",
    "    'Program': np.repeat(programs, n_employees * n_levels // n_programs),\n",
    "    'Experience': np.tile(np.repeat(experience_levels, n_employees // n_levels), n_programs)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Time ~ C(Program) + C(Experience) + C(Program):C(Experience)', data=df).fit()\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print results\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2406e-d143-48f0-be92-62ae188bc69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
